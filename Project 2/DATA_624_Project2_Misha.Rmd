---
title: "624 Final Project EDA"
author: "Jeff Shamp, Misha Kollontai, John Kellogg"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

## Intro

Working off of work started by Jeff in 
https://github.com/Shampjeff/cuny_msds/blob/master/DATA_624/final_project/final_eda.Rmd


## Data Load - Target Distribution

```{r message=FALSE, echo = FALSE, warning=FALSE}
library(tidymodels)
library(moments)
library(readxl)
library(baguette)
library(randomForest)
library(ranger)
library(kknn)
library(vip)
library(janitor)
library(Cubist)
library(rules)
sessionInfo()
```
## Intro

In order to better understand the manufacturing process within ABC Beverage, we endeavored to model the pH output within our plant based on all available predictive factors. The goal was to identify the predictors that are most indicative of the final pH value of our product as well as providing a model that sufficiently accurately predicts pH based on given values of predictors. This report could then be used to more efficiently steer manufacturing processes towards a desired pH level with (ideally) the minimum required changes to setting within the overall system.

## Data Exploration

The available data for our analysis included the output pH value along with 32 predictor variables for a set of 2,571 observations. The predictor variables include one categorical variable for the `Brand` associated with each observation; each other predictor is a numerical setting within our manufacturing process including various pressure measures, temperature settings, fill levels, etc. In a significant portion of the available data, there is a measurement missing for at least one predictor of a given observation (1,483 of 2,571). The output variable PH is fairly normally distributed around a mean of 8.55 as can be seen in the graph below.

```{r, echo = FALSE, warning = FALSE}
data <- read_excel('StudentData.xlsx') %>%
  mutate_if(is.character, factor) %>%
  rename(Brand = 'Brand Code')
#head(data)

data %>%
  ggplot(aes(x=PH)) +
  geom_histogram(binwidth = .01) +
  geom_vline(xintercept = mean(data$PH, na.rm = TRUE), linetype ='dashed',
             color = 'blue', size = 1)
```

### Outliers/Erros

Before any models could be generated from this dataset, the large number of missing data needed to be addressed. Below we can see that 3 predictors were missing data in more than 2% of observations and 11 in at least 1% of observations. 

```{r, echo = FALSE}
pct_null <- data.frame(do.call("rbind", map(data,
                                            ~ mean(is.na(.)))))
colnames(pct_null) <- c('PCT_NULL')

totalNulls <- pct_null %>%
  mutate(VARIABLE = rownames(.)) %>%
  arrange(desc(PCT_NULL)) %>%
  filter(PCT_NULL > 0) %>%
  dplyr::select(VARIABLE, PCT_NULL)

ggplot(totalNulls, aes(x = reorder(VARIABLE, PCT_NULL), y = PCT_NULL,
                       label = round(PCT_NULL*100, 1))) + 
  geom_text(vjust = 0.5, hjust = -0.05)+
  geom_bar(stat = "identity") +
  ggtitle("Variables with Missing Information") +
  xlab("Statistic") + ylab("Percent Missing") + 
  scale_y_continuous(labels=scales::percent, limits = c(0,0.1)) +  
  coord_flip() + expand_limits(y = 1)
```

Before imputing the "missing" data, we decided to look over some of the variables and determine if any of the information appeared erroneous. This would entail a datapoint where the value wasn't missing, but simply did not make sense (i.e. a negative number for a variable where that is impossible). To start we looked over how many outliers were present in the predictors. 

```{r, warning = FALSE, echo = FALSE}
totalOutliers <- data.frame(
  sapply(data %>% select(-PH,-`Brand`), 
       function(x){length(boxplot.stats(x)$out)/nrow(data)}))
totalOutliers$VARIABLE_NM <- rownames(totalOutliers)
colnames(totalOutliers) <- c('PCT_OUTLIERS', 'VARIABLE_NM')
ggplot(totalOutliers,
       aes(x = reorder(VARIABLE_NM, PCT_OUTLIERS), y=PCT_OUTLIERS,
           label = round(PCT_OUTLIERS*100, 1))) + 
  geom_text(vjust = 0.5, hjust = -0.05)+ geom_bar(stat = "identity") +
  ggtitle("Variables with Outliers") + xlab("Statistic") +
  ylab("Percent of Data that is an Outlier") + 
  scale_y_continuous(labels=scales::percent, limits = c(0,0.2)) +  
  coord_flip()
```

Though a significant number of predictors appears to contain a lot of outliers, not many of it appears to be "erroneous". A slight dig into each predictor yielded three that contained datapoints that appeared to have either been recorded in error or incorrectly entered. The three variables in question are `Mnf Flow`, `Hyd Pressure1` and `Hyd Pressure3`. For each of these 3 variables negative values don't makes sense since neither flow rate nor hydraulic pressure are unlikely to be negative by design. Each negative entry for `Mnf Flow` is at -100 as opposed to a spread of values and the negative hydraulic pressure values are all slightly below 0. 

```{r, echo = FALSE, warning = FALSE}
ggplot(data, aes(x=`Mnf Flow`, y = PH)) + geom_point()
ggplot(data, aes(x=`Hyd Pressure1`, y = PH)) + geom_point()
ggplot(data, aes(x=`Hyd Pressure3`, y = PH)) + geom_point()
```

Our solution to the seemingly erroneous data was to convert negative Hydraulic Pressure and 'Mnf Flow' readings with NA's so they wiould be imputed along with the information that was missing from the get-go.

```{r, echo = FALSE}
data <- data %>%
  mutate(`Mnf Flow` = replace(`Mnf Flow`, `Mnf Flow`< 10, NA)) %>%
  mutate(`Hyd Pressure1` = replace(`Hyd Pressure1`, `Hyd Pressure1`< 0, NA)) %>%
  mutate(`Hyd Pressure3` = replace(`Hyd Pressure3`, `Hyd Pressure3`< 0, NA))
```

### Imputation

In order to prepare the data for modeling we first split the data into training/testing sets along an 80/20 percentage split, and utilized the `tidymodels` package within R to create what is known as a `recipe`. This `recipe` outlines the following steps that are then followed in order to pre-process a given dataset:

1. Specifies $PH$ as the predicted variable within the dataset
2. Centers all numeric predictors, normalizing them around 0
3. Performs a BoxCox transformation on numeric predictors
4. Runs a k-Nearest Neighbor imputation on all missing data (NAs)
5. converts all nominal predictors (`Brand`) into dummy variables
6. Removes observations for which the predicted variable (`PH`) is missing

```{r, echo = FALSE}
set.seed(312)
data_split <-initial_split(data, prop=.80)
train_data <- training(data_split)
test_data <- testing(data_split)

ph_recipe <-
  recipe(PH ~ ., data=data) %>%
  #update_role(`Brand Code`, new_role = "brand") %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_BoxCox(all_numeric(), -all_outcomes()) %>%
  step_impute_knn(all_predictors()) %>%
  step_dummy(all_nominal()) %>%
  step_naomit(all_outcomes())

ph_training <- ph_recipe %>%
  prep() %>%
  juice()

ph_testing <- ph_recipe %>%
  prep() %>%
  bake(test_data)
```

## Model Creation

After identifying erroneous data, replacing it with NAs and specifying the pre-processing approach, several models were generated. The models created and evaluated against the training data are as follows:

* Linear Regression
* Random Forest (randomForest engine)
* Random Forest (ranger engine)
* k-Nearest Neighbors
* Cubist

```{r, echo = FALSE}
####Linear Regression Model##########################################
lr_model <- linear_reg() %>%
  set_engine("lm") %>%
  fit(PH ~ ., ph_training)

###Random Forest 'randomForest' Model################################
rf_model <- rand_forest(trees = 100, mode = "regression") %>%
  set_engine("randomForest") %>%
  fit(PH ~ ., ph_training)

###Random Forest 'ranger' Model######################################
ranger_model <- rand_forest(trees = 100, mode = "regression") %>%
  set_engine("ranger") %>%
  fit(PH ~ ., ph_training)

###k-Nearest Neighbors Model#########################################
knn_model <- nearest_neighbor(neighbors = 10) %>%
  set_mode(mode = "regression") %>%
  set_engine("kknn") %>%
  fit(PH ~., ph_training)

###Cubist Model######################################################
cubist_model <- cubist_rules(committees = 10, neighbors = 2) %>%
  set_engine("Cubist") %>%
  fit(PH ~., ph_training)
```

```{r, echo = FALSE}
model_metrics <- function(models,test_recipe){
  mod_methods <- c()
  mod_metrics <- NULL %>%
    rbind(c('RMSE','RSQ','MAE'))
  for (model in models){
    mod_methods <- c(mod_methods,model$spec$engine)
    prediction <- predict(model,test_recipe)
    pred_metrics <- prediction %>%
      bind_cols(test_recipe) %>%
      metrics(truth = PH, estimate = .pred) %>%
      pull(.estimate)
    mod_metrics <- rbind.data.frame(mod_metrics,round(pred_metrics,4))
  }
  mod_metrics <- mod_metrics %>%
    row_to_names(row_number = 1)
  rownames(mod_metrics) <- mod_methods
  
  return(mod_metrics)
}
```

The models were fit to the training dataset and then each was evaluated against the test set for accuracy metrics. Below is a table summarizing 3 performance metrics for each of the 5 evaluated models: Root Mean Square Error (RMSE), R-Squared (RSQ) and Mean Absolute Error (MAE). 

```{r, echo = false}
models <- list(lr_model,rf_model,ranger_model,knn_model,cubist_model)
(model_test <- model_metrics(models,ph_testing))
```

Based on each of the three performance metrics, the Cubist model appears to be the most accurate in predicting our test data. For each model a set of parameters was taken for the initial model at random - since the Cubist model appears to be the most accurate, a tuning algorithm was applied to this model in order to identify optimal parameters. The tuning was performed across a wide range of the two input parameters ($committees$, $neighbors$); below is a plot visualizing only the final stage of this refinement in order to simplify the factors at play:

```{r}
full_cube_grid <- expand.grid(committees = 5:35, neighbors = c(1, 7, 9))
sub_cube_grid <- expand.grid(committees = 5:15, neighbors = c(2, 7))
set.seed(44)
folds <- vfold_cv(train_data)

cubist_model <- cubist_rules(committees = tune(), neighbors = tune()) %>%
  set_engine("Cubist")

cube_tune_res <- cubist_model %>%
  tune_grid(PH ~ ., resamples = folds, grid = sub_cube_grid)
```

```{r}
cube_tune_res %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  mutate(neighbors = factor(neighbors)) %>%
  ggplot(aes(x = committees, y = mean, col = neighbors)) +
  geom_point() +
  geom_line() +
  scale_color_brewer(palette = "Dark2") +
  theme(legend.position = "top")

cube_tune_res %>%
  collect_metrics() %>%
  filter(.metric == "rsq") %>%
  mutate(neighbors = factor(neighbors)) %>%
  ggplot(aes(x = committees, y = mean, col = neighbors)) +
  geom_point() +
  geom_line() +
  scale_color_brewer(palette = "Dark2") +
  theme(legend.position = "top")
```


```{r}
show_best(cube_tune_res, metric = "rmse")
smallest_rmse <- select_best(cube_tune_res, metric = "rmse")
final_cubist_model <-
  cubist_model %>%
  finalize_model(smallest_rmse) %>%
  fit(PH ~., ph_training)

cubist_model_tuned <- cubist_rules(committees = 12, neighbors = 7) %>%
  set_engine("Cubist") %>%
  fit(PH ~., ph_training)

```

```{r}
models <- list(lr_model,rf_model,ranger_model,knn_model, final_cubist_model)
(model_test <- model_metrics(models,ph_testing))
```

Tuning the model in terms of the Committee and Neighbors parameters does not seem to improve the performance against our test set, suggesting some overfitting to the training data. We settled on using the original cubist model to move forward with our predictions. 

