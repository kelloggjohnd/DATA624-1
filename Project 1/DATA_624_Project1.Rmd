---
title: "DATA624 Project 1"
author: "M Kollontai"
date: "3/18/2021"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(fpp2)
library(mlbench)
library(ggplot2)
library(tidyr)
library(openxlsx)
library(readxl)
library(dplyr)
```

# **Part A - ATM Forecast** 

In part A, I want you to forecast how much cash is taken out of 4 different ATM machines for May 2010.  The data is given in a single file.  The variable ‘Cash’ is provided in hundreds of dollars, other than that it is straight forward.   I am being somewhat ambiguous on purpose to make this have a little more business feeling.  Explain and demonstrate your process, techniques used and not used, and your actual forecast.  I am giving you data via an excel file, please provide your written report on your findings, visuals, discussion and your R code via an RPubs link along with the actual.rmd file  Also please submit the forecast which you will put in an Excel readable file.

## Data Exploration

```{r}
ATMs <- read_excel("ATM624Data.xlsx" ) %>%
  mutate(DATE = as.Date(DATE, origin = "1899-12-30"))
summary(ATMs)
```

We can see that there are 19 entries with missing data in the `Cash` column. Let's view these entries and see what needs to be done with them:

```{r}
ATMs[is.na(ATMs$Cash),]
```

As we can see, 14 of the 19 entries with missing are actually placeholders for the May 2010 data we are meant to predict - these can be removed in order to simplify analysis of the dataset. The other 5 entries with NA's all occurred in June of 2019 and pertain to ATM1 or ATM2. These will need to be imputed before any prediction can be made. 

```{r}
date_end <- "2010-05-01"
ATMs <- ATMs[ATMs$DATE < date_end,]
ATMs$Day <- weekdays(ATMs$DATE)
ATMs[is.na(ATMs$Cash),]
```

In order to determine the best path of imputation, let's split up the single dataset into separate dataframes - one for each ATM.

```{r}
ATM_1 <- ATMs[ATMs$ATM == 'ATM1',c('DATE','Cash','Day')]
ATM_2 <- ATMs[ATMs$ATM == 'ATM2',c('DATE','Cash','Day')]
ATM_3 <- ATMs[ATMs$ATM == 'ATM3',c('DATE','Cash','Day')]
ATM_4 <- ATMs[ATMs$ATM == 'ATM4',c('DATE','Cash','Day')]
```

## Predictions {.tabset .tabset-pills}

### ATM1 

```{r}
ggplot(ATM_1,aes(x=DATE, y =Cash)) + geom_point()
```

Looking over a scatter plot of the existing data from ATM1, we can see that the data appears to fall into two groups - one centered around 90 and the other around 15. Let's take a look and see if the lower numbers are tied to a particular day of the week.

```{r}
ATM_1_noNA <- ATM_1[!is.na(ATM_1$Cash),]
ATM_1_byDay <- aggregate(ATM_1_noNA[,2],list(ATM_1_noNA$Day),mean)
ggplot(ATM_1_byDay, aes(x=Group.1, y = Cash)) +
  geom_bar(stat='identity') +
  ggtitle('Average withdrawal from ATM1 by Day of Week') +
  xlab('Day of Week') +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
ggplot(ATM_1, aes(x=DATE, y = Cash)) +
  geom_point() +
  facet_wrap(~Day) +
  xlab('') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

For some reason we see that Thursday shows much fewer withdrawals at this particular ATM until about February 2010, whereas Tuesday exhibits a somewhat opposite trend. One possibility is that the operating hours of the ATM were much lower on Thursdays until a particular date, and this reduction of hours was changed to Tuesdays on that date. Wednesday appears to be the only other day of the week that shows a change in the trend on that day, with a spike in the data on the day followed by slightly higher data afterwards, though not nearly as dramatic a change as we see for the other 2 days. Other interesting datapoints include some extremely low datapoints on 3 Fridays, 1 Monday, 1 Sunday and 1 Wednesday. 

As we saw above, all of the missing datapoints for ATM1 were from June 2009 - a Saturday, a Monday and a Tuesday. Let's impute the missing datapoints with the averages of the day of week for Saturday and Monday - as they seem to exhibit a fairly flat trend, and with the average of the first cluster of data (before the shift to lower withdrawals) for the Tuesday datapoint.

```{r}
ATM_1_Sat <- ATM_1_byDay[ATM_1_byDay$Group.1 == "Saturday",'Cash']
ATM_1_Mon <- ATM_1_byDay[ATM_1_byDay$Group.1 == "Monday",'Cash']
first_tue_ATM1 <- ATM_1[(ATM_1$Day=='Tuesday' & ATM_1$DATE < "2010-02-01" ),]
ATM_1_Tue <- mean(first_tue_ATM1$Cash, na.rm = TRUE)

ATM_1[ATM_1$DATE == '2009-06-13','Cash'] <- ATM_1_Sat
ATM_1[ATM_1$DATE == '2009-06-22','Cash'] <- ATM_1_Mon
ATM_1[ATM_1$DATE == '2009-06-16','Cash'] <- ATM_1_Tue
ggplot(ATM_1,aes(x=DATE, y =Cash)) + geom_point()
```

Due to the dramatic shift in some of the trends, let's identify when the shift in trend occurred by plotting the data around the date in question:

```{r}
library(scales)
ggplot(ATM_1,aes(x=DATE, y =Cash, color = Day)) + 
  geom_point() +
  scale_x_date(labels = date_format("%m/%d"), 
               date_breaks = '2 days', 
               limits = as.Date(c('2010-02-01','2010-03-01')))
```

Based on the graph above we can see that 2/17 is the date where the Wednesday spike happened and the shift in Tuesday and Thursday trends occurred. The data before this data doesn't appear to have much impact on the trend after it, so we can simply use the data from after 2/17 as the baseline for our prediction:

```{r}
ATM_1_sub <- ATM_1[ATM_1$DATE > "2010-02-17",]
ggplot(ATM_1_sub,aes(x=DATE, y =Cash)) + 
  geom_point(aes(color = Day)) +
  geom_line() +
  scale_x_date(labels = date_format("%m/%d"), 
               date_breaks = '7 days')
```
We will forecast 31 days into the future to predict data for May 2010:

```{r}
ATM_1_ts <- ATM_1_sub$Cash %>%
  ts(start = 1, frequency = 7) 
ATM_1_fit <- ATM_1_ts %>% auto.arima(lambda = 0)
ATM_1_fore <- forecast(ATM_1_fit, h =31)
autoplot(ATM_1_fore)
```

```{r}
DATE <- seq(as.Date("2010-05-01"), as.Date("2010-05-31"), by="days")
Cash <- ATM_1_fore$mean
ATM_1_May2010 <- data.frame(DATE, Cash)
ATM_1_May2010$Day <- weekdays(ATM_1_May2010$DATE)
ATM_1_pred <- rbind(ATM_1,ATM_1_May2010)
write.xlsx(ATM_1_May2010, 'ATM1_prediction.xlsx')
write.xlsx(ATM_1_pred, 'ATM1_full.xlsx')
```

### ATM2

```{r}
ggplot(ATM_2,aes(x=DATE, y =Cash)) + geom_point()
```

Looking over a scatter plot of the existing data from ATM2, we can see that the data appears to be far more evenly (randomly) distributed than the data for ATM1 was. et's take a look at individual days of the week to determine any sub-trends there. 

```{r}
ATM_2_noNA <- ATM_2[!is.na(ATM_2$Cash),]
ATM_2_byDay <- aggregate(ATM_2_noNA[,2],list(ATM_2_noNA$Day),mean)
ggplot(ATM_2_byDay, aes(x=Group.1, y = Cash)) +
  geom_bar(stat='identity') +
  ggtitle('Average withdrawal from ATM2 by Day of Week') +
  xlab('Day of Week') +
  theme(plot.title = element_text(hjust = 0.5))
```

There are obvious differences between the days of the week, suggesting "seasonality" based on the time of the week. 

```{r}
ggplot(ATM_2, aes(x=DATE, y = Cash)) +
  geom_point() +
  facet_wrap(~Day) +
  xlab('') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

As with the data for ATM1, we see that there is a date within the data that separates two very different trends; the days most strongly affected appear to be Monday, Tuesday, Wednesday and Thursday. 

Both missing datapoints for ATM2 were again from June 2009 as with ATM1 (perhaps the data collection method used for this dataset malfunctioned in this month) - a Wednesday, and a Thursday. Since we see similar behavior in the data as we did for ATM1, let's impute the missing datapoints with the averages of the day of week for the first cluster of data (before the shift in withdrawal numbers).

```{r}
first_wed_ATM2 <- ATM_2[(ATM_2$Day=='Wednesday' & ATM_2$DATE < "2010-02-01" ),]
ATM_2_Wed <- mean(first_wed_ATM2$Cash, na.rm = TRUE)
first_thr_ATM2 <- ATM_2[(ATM_2$Day=='Thursday' & ATM_2$DATE < "2010-02-01" ),]
ATM_2_Thr <- mean(first_thr_ATM2$Cash, na.rm = TRUE)

ATM_2[ATM_2$DATE == '2009-06-24','Cash'] <- ATM_2_Wed
ATM_2[ATM_2$DATE == '2009-06-18','Cash'] <- ATM_2_Thr
```

Let's now identify if the shift in trend occurred around the same date as it did for ATM1:

```{r}
library(scales)
ggplot(ATM_2[(ATM_2$Day=='Monday' | ATM_2$Day=='Tuesday' | ATM_2$Day=='Wednesday' | ATM_2$Day=='Thursday'),],aes(x=DATE, y =Cash, color = Day)) + 
  geom_point() +
  scale_x_date(labels = date_format("%m/%d"), 
               date_breaks = '2 days', 
               limits = as.Date(c('2010-02-01','2010-03-01')))
```

Again, we can see here that June ${17}_{th}$ is when the shift in the trend occurred. Since there is such a dramatic change in trends after this date, we will again only use the data after the shit to form our prediction. 

```{r}
ATM_2_sub <- ATM_2[ATM_2$DATE > "2010-02-17",]
ggplot(ATM_2_sub,aes(x=DATE, y =Cash)) + 
  geom_point(aes(color = Day)) +
  geom_line() +
  scale_x_date(labels = date_format("%m/%d"), 
               date_breaks = '7 days')

ATM_2_ts <- ATM_2_sub$Cash %>%
  ts(start = 1, frequency = 7) 
#ATM_2_fit <- ATM_2_ts %>% auto.arima(lambda = 0)
#We see here that no appropriate ARIMA model is found - let's use a Holt-Winters' model. 
ATM_2_fit2 <- hw(ATM_2_ts, h =31, damped=TRUE)
ATM_2_fore <- forecast(ATM_2_fit2, h =31)
autoplot(ATM_2_fore)
```

```{r}
DATE <- seq(as.Date("2010-05-01"), as.Date("2010-05-31"), by="days")
Cash <- ATM_2_fore$mean
ATM_2_May2010 <- data.frame(DATE, Cash)
ATM_2_May2010$Day <- weekdays(ATM_2_May2010$DATE)
ATM_2_pred <- rbind(ATM_2,ATM_2_May2010)
write.xlsx(ATM_2_May2010, 'ATM2_prediction.xlsx')
write.xlsx(ATM_2_pred, 'ATM2_full.xlsx')
```

### ATM3

```{r}
ggplot(ATM_3,aes(x=DATE, y =Cash)) + geom_point()
```

```{r}
ggplot(ATM_3[ATM_3$DATE > '2010-04-20',],aes(x=DATE, y =Cash)) + geom_point()
```

We can see here that ATM3 contains zero data for everything but the last 3 observations. A likely explanation for this is that the ATM simply wasn't operational until April 28th. Three datapoints, especially for a set of data that is theoretically seasonal over a week is simply not enough to generate a viable model. The simplest approach would be to simply generate a naive forecast:

```{r}
fit3_1 <- naive(ATM_3[ATM_3$DATE > '2010-04-20','Cash'], h=31)
autoplot(forecast(fit3_1))
```

This approach clearly shows an incredible degree of uncertainty relative to the amounts involved, and we saw from the previous 2 ATMs analyzed that the actual withdrawal number vary greatly, spanning from nearly zero on a given day to whatever the maximum values observed are. Perhaps there are some similarities between the numbers observed here and the previously analyzed ATMs?

```{r}
ATMs[ATMs$DATE > '2010-04-27',] %>%
  spread('ATM', 'Cash')
```

We can see that the withdrawal amounts observed at ATM3 for the three dates in question match exactly the numbers wee saw for ATM1, albeit only for the 3 days available. It may be best to simply use our prediction for ATM1 as the prediction for ATM3, though it must be made clear to anyone using this data that the prediction is based on similarities between 3 data points and should therefore be taken with a boulder-sized grain of salt. Ideally much more data should be collected from this ATM before any predictions are made. 

```{r}
ATM_3_pred <- ATM_1_pred
write.xlsx(ATM_1_May2010, 'ATM3_prediction.xlsx')
write.xlsx(ATM_3_pred, 'ATM3_full.xlsx')
```

### ATM4

```{r}
ggplot(ATM_4,aes(x=DATE, y =Cash)) + geom_point()
```

We can clearly see an outlier in the data that needs to be adjusted. We will use the median of the data for that weekday to replace the (most likely) erroneous datapoint. 

```{r}
ATM_4 %>% 
  arrange(desc(Cash)) %>%
  head(5)
ATM_4_mean_Tue <- median(ATM_4$Cash[(ATM_4$Day == 'Tuesday' & ATM_4$DATE != '2010-02-09')], na.rm = TRUE)
ATM_4[ATM_4$DATE == '2010-02-09', 'Cash'] <- ATM_4_mean_Tue
ggplot(ATM_4,aes(x=DATE, y =Cash)) + geom_point()
```

```{r}
ATM_4_byDay <- aggregate(ATM_4[,2],list(ATM_4$Day),mean)
ggplot(ATM_4_byDay, aes(x=Group.1, y = Cash)) +
  geom_bar(stat='identity') +
  ggtitle('Average withdrawal from ATM4 by Day of Week') +
  xlab('Day of Week') +
  theme(plot.title = element_text(hjust = 0.5))
```

Can we see steady trends within the days of teh week that would help us generate a stable model?

```{r}
ggplot(ATM_4, aes(x=DATE, y = Cash)) +
  geom_point() +
  facet_wrap(~Day) +
  xlab('') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Again we can see a dramatic shift in the data for a couple of the days of the week - in this case Tuesday and Thursday; let's identify the breakpoint.

```{r}
ggplot(ATM_4[(ATM_4$Day=='Tuesday' | ATM_4$Day=='Thursday'),],aes(x=DATE, y =Cash, color = Day)) + 
  geom_point() +
  scale_x_date(labels = date_format("%m/%d"), 
               date_breaks = '4 days', 
               limits = as.Date(c('2010-01-20','2010-03-10')))
```

With the information we have on previous ATMs, the $17_{th}$ seems to again be the breakpoint day. As before, we will generate a prediction based on the data after this date. 

```{r, warning= FALSE}
ATM_4_sub <- ATM_4[ATM_4$DATE > '2010-02-17',]
ggplot(ATM_4_sub,aes(x=DATE, y =Cash)) + 
  geom_point(aes(color = Day)) +
  geom_line() +
  scale_x_date(labels = date_format("%m/%d"), 
               date_breaks = '7 days')
```

There does appear to be a slight decrease in the magnitude of the seasonal aspect within our data, but other than that, patterns are difficult to identify - Tuesdays consistently show extremely low withdrawals, but the rest of the data almost appears random. Sunday in particular is interesting as it has both some of the highest and lowest points within this dataset. Let's use the `auto.arima` function again and see what the predictions look like. 

```{r}
ATM_4_ts <- ATM_4_sub$Cash %>%
  ts(start = 1, frequency = 7) 
ATM_4_fit <- ATM_4_ts %>% auto.arima(lambda = 0)
ATM_4_fore <- forecast(ATM_4_fit, h =31)
autoplot(ATM_4_fore)
```


This prediction has extremely high error margins, but the mean projection itself seems reasonable - we will use this as our prediction for ATM4. The prediction accounts for what seems to be an overall decrease in the seasonality of the data. Much more long term this is probably inaccurate as it would trend towards no withdrawals at all; realistically they are probably trending downwards towards some stable value above zero. 

```{r}
DATE <- seq(as.Date("2010-05-01"), as.Date("2010-05-31"), by="days")
Cash <- ATM_4_fore$mean
ATM_4_May2010 <- data.frame(DATE, Cash)
ATM_4_May2010$Day <- weekdays(ATM_4_May2010$DATE)
ATM_4_pred <- rbind(ATM_4,ATM_4_May2010)
write.xlsx(ATM_4_May2010, 'ATM4_prediction.xlsx')
write.xlsx(ATM_4_pred, 'ATM4_full.xlsx')
```


# **Part B - Forecasting Power** 

Part B consists of a simple dataset of residential power usage for January 1998 until December 2013.  Your assignment is to model these data and a monthly forecast for 2014.  The data is given in a single file.  The variable ‘KWH’ is power consumption in Kilowatt hours, the rest is straight forward.    Add this to your existing files above. 

```{r}
Power <- read_excel("ResidentialCustomerForecastLoad-624.xlsx") %>%
  rename(DATE = 'YYYY-MMM') %>%
  mutate(DATE = as.Date(paste0('15-', DATE), '%d-%Y-%b'))

plot_pow_ts <- function(df){
  ggplot(df,aes(x=DATE, y =KWH)) + 
  geom_line() +
  scale_x_date(labels = date_format("'%y"), 
               date_breaks = '1 year') +
  ggtitle('Monthly Residential Power Usage') +
  ylim(0,12000000) +
  xlab('Year')
}
plot_pow_ts(Power)
```

There does appear to be one outlier in the year 2010 - let's look at individual monthly data breakdowns to see when it occurred. 

```{r}
Power$Month <- months(Power$DATE)
plot_months <- function(df){
  ggplot(df, aes(x=DATE, y = KWH)) +
  geom_point() +
  facet_wrap(~Month) +
  xlab('') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
}
plot_months(Power)
```

We can see some fairly even trends within the data, with the single outlier obvious within the July data. Are there any NAs on top of that?

```{r}
Power[Power$Month == 'July',]
Power[is.na(Power$KWH),]
```

We can see that the outlier in 2010 appears to be a factor of 10 off from the rest - it is possible that the data was mistakenly reduced by a factor of 10. One approach we can take to "fix" the outlier is to simply multiply this datapoint by 10. The NA occurred in September 2008 - the years on either side indicated a fairly even rising trend, so we can backfill the data as the average of the data from 2007 and 2009. 

```{r}
Power[Power$DATE == '2010-07-15','KWH'] <- Power[Power$DATE == '2010-07-15','KWH']*10
Power[Power$DATE == '2008-09-15','KWH'] <- (Power[Power$DATE == '2007-09-15','KWH']+Power[Power$DATE == '2007-09-15','KWH'])/2

plot_months(Power)
```

```{r}
plot_pow_ts(Power)
```

Now that we have a relatively clean timeseries, we can attempt to use the `auto-arima` function to model a prediction for 2014. 

```{r}
power_ts <- Power$KWH %>% 
  ts(frequency = 12, start=c(1998,1))
power_fit <- auto.arima(power_ts)
power_fore <- forecast(power_fit, h =12)
autoplot(power_fore)
```

```{r}
checkresiduals(power_fit)
```

The fairly large p-values suggest the residuals resemble white noise; the strongly normal distribution of these residuals inspires confidence as well. Let us export these predictions.

```{r}
DATE <- seq(as.Date("2014-01-15"), as.Date("2014-12-15"), by="months")
KWH <- power_fore$mean
Power_2014 <- data.frame(DATE, KWH)
Power_2014$Month <- months(Power_2014$DATE)
Power_pred <- rbind(Power[,c(2,3,4)],Power_2014)
write.xlsx(Power_2014, 'Power_prediction.xlsx')
write.xlsx(Power_pred, 'Power_full.xlsx')
```

# **Part C - BONUS** {.tabset .tabset-pills}

Part C consists of two data sets. These are simple 2 columns sets, however they have different time stamps. Your optional assignment is to time-base sequence the data and aggregate based on hour (example of what this looks like, follows).Note for multiple recordings within an hour, take the mean. Then to determine if the data is stationary and can it be forecast. If so, provide a week forward forecast and present results via Rpubs and .rmd and the forecast in an Excel readable file.

## Data Exploration

```{r}
library(lubridate)
Pipe1 <- read_excel("Waterflow_Pipe1.xlsx") %>%
  rename(DATE = 'Date Time') %>%
  mutate(DATE = as.POSIXct(DATE*(60*60*24), origin = "1899-12-30"))
Pipe1$DATE_HR <- format(Pipe1$DATE,format = "%Y-%m-%d %H")
#Pipe1$DATE_HR <- as.POSIXct(Pipe1$DATE, format = "%Y-%m-%d %H")
Pipe1$HR <- format(Pipe1$DATE,format = "%H")

Pipe1_Hrly <- Pipe1 %>%
  group_by(DATE_HR) %>%
  select(WaterFlow, DATE_HR, HR) %>%
  summarise(Mean_Hr_Flow = mean(WaterFlow, na.rm = TRUE), Readings = n(), HR = first(HR)) %>%
  mutate(Pipe = '1')
```

```{r}
Pipe2 <- read_excel("Waterflow_Pipe2.xlsx") %>%
  rename(DATE = 'Date Time') %>%
  mutate(DATE = as.POSIXct(DATE*(60*60*24), origin = "1899-12-30"))
Pipe2$DATE_HR <- format(Pipe2$DATE,format = "%Y-%m-%d %H")
#Pipe2$DATE_HR <- as.POSIXct(Pipe2$DATE, format = "%Y-%m-%d %H")
Pipe2$HR <- format(Pipe2$DATE,format = "%H")

Pipe2_Hrly <- Pipe2 %>%
  group_by(DATE_HR) %>%
  select(WaterFlow, DATE_HR, HR) %>%
  summarise(Mean_Hr_Flow = mean(WaterFlow, na.rm = TRUE), Readings = n(), HR = first(HR)) %>%
  mutate(Pipe = '2')
```


```{r}
Pipes_Hrly <- rbind(Pipe1_Hrly,Pipe2_Hrly)
Pipes_Hrly$DATE_HR <- as.POSIXct(Pipes_Hrly$DATE_HR, format = "%Y-%m-%d %H")
ggplot(Pipes_Hrly,aes(x=DATE_HR, y = Mean_Hr_Flow)) + 
  geom_point(aes(color = Pipe)) +
  ggtitle('Average Hourly Waterflow by Pipe') +
  xlab('Date') +
  ylab('Water Flow Rate') + 
  scale_x_datetime(labels = date_format("%m-%d"), date_breaks = '4 days')
```

We can see from the plot above that while both datasets contain the same amount of datapoints, Pipe 1 contains information collected over approximately 10 days, whereas the data for Pipe2 spans a little over a month worth of data. The data for Pipe1 is also only aggregated to 236 hours worth of datapoints, whereas Pipe2 has 667 hours worth of datapoints. 

The data looks largely random when viewing it as a whole, but are there any patterns? Perhaps looking at the data hourly would show something? Let's look at each pipe individually since there seems to be little to suggest they are the same pipe. 

## Pipe 1

```{r}
Pipe1_Hrly$DATE_HR <- as.POSIXct(Pipe1_Hrly$DATE_HR, format = "%Y-%m-%d %H")
ggplot(Pipe1_Hrly, aes(x=DATE_HR, y = Mean_Hr_Flow)) +
  geom_line() +
  facet_wrap(~HR) +
  xlab('') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  scale_x_datetime(labels = date_format("%m-%d"), date_breaks = '2 days')
```

```{r}
library(xts)
library(tsbox)
pipe1_xts <- xts(Pipe1_Hrly$Mean_Hr_Flow, order.by=Pipe1_Hrly$DATE_HR)
pipe1_ts <- ts_ts(pipe1_xts)

autoplot(pipe1_ts)
```

```{r}
(Pipe1_fit <- pipe1_ts %>% arima(order=c(3,1,1)))
(Pipe1_fore <- forecast(Pipe1_fit, h =24*7))

checkresiduals(Pipe1_fit)
(forecast(Pipe1_fit))
```

## Pipe 2

```{r}
Pipe2_Hrly$DATE_HR <- as.POSIXct(Pipe2_Hrly$DATE_HR, format = "%Y-%m-%d %H")
ggplot(Pipe2_Hrly, aes(x=DATE_HR, y = Mean_Hr_Flow)) +
  geom_line() +
  facet_wrap(~HR) +
  xlab('') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  scale_x_datetime(labels = date_format("%m-%d"), date_breaks = '5 days')
```

```{r}
pipe2_xts <- xts(Pipe2_Hrly$Mean_Hr_Flow, order.by=Pipe2_Hrly$DATE_HR)
pipe2_ts <- ts_ts(pipe2_xts)

autoplot(pipe2_ts)
(Pipe2_fit <- pipe2_ts %>% arima(order=c(3,1,1)))
(Pipe2_fore <- forecast(Pipe2_fit, h =24*7))

checkresiduals(Pipe2_fit)

```

```{r}

write.xlsx(Pipe1_fore$mean, 'Pipe1_Prediction.xlsx')
write.xlsx(Pipe2_fore$mean, 'Pipe2_Prediction.xlsx')
```